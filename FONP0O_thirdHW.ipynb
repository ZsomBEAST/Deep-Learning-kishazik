{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harmadik_kishazi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWSQsEDgyipqfnnLeiKohz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsomBEAST/Deep-Learning-kishazik/blob/main/FONP0O_thirdHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bZUf52Hwy_O"
      },
      "source": [
        "from tensorflow.keras.models import Sequential                  #imports to make keras model\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import Callback \n",
        "from tensorflow.keras.optimizers import SGD #we will use stohastic gradient \n",
        "import numpy as np\n",
        "from keras.models import load_model  #we will save the best model and in the end reproduce it\n",
        "from sklearn import preprocessing #for normalization\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "import csv # for reading csv\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpigQOjcasyX",
        "outputId": "4aa90d3b-276f-4187-d474-948803b19997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predfrom=365 #we can set how much previous temperatuers are the input -> I chose 1 year beacuse i want the network to recognise the changing of the seasons\n",
        "predto=31 # the out put is 31 days = 1 moth\n",
        "avgT=[] #list for the database\n",
        "with open('hom_csv.csv') as csv_file: #load the csv (I have loaded all the data from 'http://idojarasbudapest.hu/archivalt-idojaras' to a CSV file)\n",
        "    csv_reader = csv.reader(csv_file, delimiter=';') # ';' is the separating character\n",
        "    line_count = 0 # line counter\n",
        "    for row in csv_reader:\n",
        "        if(not(line_count%2)): #the tempreatures are in every 2nd row\n",
        "            avgT.append((float(row[1])+float(row[2]))/2) #mean=(min+max)/2\n",
        "            #print(row[1],row[2])\n",
        "        line_count=line_count+1\n",
        "\n",
        "#we predict the next 31 day from the last 60 day\n",
        "avgT_array=np.array(avgT) #make np.array from the list\n",
        "x_all=[]\n",
        "y_all=[] #lists for the input and output datas of the netowrk\n",
        "#making the inputs and the labels\n",
        "for i in range(predfrom,(len(avgT_array)-predto+1)):\n",
        "    x_all.append(avgT_array[(i-predfrom):i]) #the input is the pervious 365 day\n",
        "    y_all.append(avgT_array[i:(i+predto)]) #the label for the cost is the next 31 day\n",
        "\n",
        "x_test_pre=avgT_array[-predfrom-1:-1] # we will test the prediction the the future, what does it predict for the next month (ofc we are not able to calculate the cost right now)\n",
        "\n",
        "print(x_test_pre.shape) #the last year\n",
        "x_test=np.zeros([1,predfrom]) #make a 2D array from the 1D array to match the networks input nd the\n",
        "x_test[0]=x_test_pre \n",
        "print(x_test.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(365,)\n",
            "(1, 365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8K4EUJTas1U",
        "outputId": "20ad39d3-cf57-402b-9931-8f68a99231c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(len(avgT_array)) #the length of the original avarage temperature dataset\n",
        "print(len(x_all)) #length of the netowrks input dataset\n",
        "print(len(y_all)) #length of the netowrks expectation dataset\n",
        "\n",
        "x_all=np.array(x_all) #We had a list with 1D np.array elemnts so far. Now we make 2D array from that.\n",
        "y_all=np.array(y_all)\n",
        "\n",
        "#make the order of the array random ->we dont want that the order influence the learning\n",
        "randperm = np.random.permutation(len(x_all))\n",
        "x_all, y_all = x_all[randperm], y_all[randperm]\n",
        "\n",
        "#split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=0.2, shuffle=True) # 80% of the data is the train data, 20% is for validation\n",
        "#the test will be the future predict:)\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(x_train) #normalazie all the inputs by the x_train\n",
        "x_train = scaler.transform(x_train)\n",
        "x_valid = scaler.transform(x_valid)\n",
        "x_test= scaler.transform(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_valid.shape)\n",
        "print(y_valid.shape)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3278\n",
            "2883\n",
            "2883\n",
            "(2306, 365)\n",
            "(2306, 31)\n",
            "(577, 365)\n",
            "(577, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKZ4_-t4ssb",
        "outputId": "5135b17b-1876-47aa-dde9-067a8a22ca7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "patience=20\n",
        "early_stopping=EarlyStopping(patience=patience, verbose=1) #we are using earlystoping, the laerning usually stops the intesive learning about 250- 30 epochs\n",
        "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=1) #we save the networks wights when the cost is the smallest\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10000, input_shape=(predfrom,))) #we use only 1 hiden layer with 10000 neuron, I have tried lots of variations, this variation has got the best loss/source in my opp.\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) # we apply dropout as well\n",
        "model.add(Dense(predto, activation='linear'))\n",
        "\n",
        "\n",
        "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) # set the erst of the hyperparameters (by iterations)\n",
        "model.compile(loss='mse', optimizer=sgd) \n",
        "history=model.fit(x_train,y_train,epochs=10000, #we say 10000 epoch but it usually earlystops around 300 \n",
        "                  batch_size=10, # we use very small batches\n",
        "                  verbose=2,\n",
        "                  validation_data=(x_valid, y_valid),\n",
        "                  callbacks=[checkpointer, early_stopping])\n",
        "\n",
        "\n",
        "model = load_model('weights.hdf5') #reproduce the best model "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 14.28726, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 22.2688 - val_loss: 14.2873\n",
            "Epoch 2/10000\n",
            "\n",
            "Epoch 00002: val_loss improved from 14.28726 to 13.47603, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 14.0492 - val_loss: 13.4760\n",
            "Epoch 3/10000\n",
            "\n",
            "Epoch 00003: val_loss improved from 13.47603 to 13.04885, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 13.2919 - val_loss: 13.0489\n",
            "Epoch 4/10000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 13.04885\n",
            "231/231 - 1s - loss: 12.8680 - val_loss: 13.0646\n",
            "Epoch 5/10000\n",
            "\n",
            "Epoch 00005: val_loss improved from 13.04885 to 12.40481, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 12.4877 - val_loss: 12.4048\n",
            "Epoch 6/10000\n",
            "\n",
            "Epoch 00006: val_loss improved from 12.40481 to 12.39091, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 12.3200 - val_loss: 12.3909\n",
            "Epoch 7/10000\n",
            "\n",
            "Epoch 00007: val_loss improved from 12.39091 to 11.95367, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 12.0787 - val_loss: 11.9537\n",
            "Epoch 8/10000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 11.95367\n",
            "231/231 - 1s - loss: 11.8321 - val_loss: 12.0479\n",
            "Epoch 9/10000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 11.95367\n",
            "231/231 - 1s - loss: 11.5415 - val_loss: 12.2519\n",
            "Epoch 10/10000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 11.95367\n",
            "231/231 - 1s - loss: 11.3971 - val_loss: 12.6773\n",
            "Epoch 11/10000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 11.95367\n",
            "231/231 - 1s - loss: 11.3024 - val_loss: 12.3479\n",
            "Epoch 12/10000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 11.95367\n",
            "231/231 - 1s - loss: 10.9960 - val_loss: 12.7173\n",
            "Epoch 13/10000\n",
            "\n",
            "Epoch 00013: val_loss improved from 11.95367 to 11.58156, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 10.7779 - val_loss: 11.5816\n",
            "Epoch 14/10000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 11.58156\n",
            "231/231 - 1s - loss: 10.5129 - val_loss: 11.5826\n",
            "Epoch 15/10000\n",
            "\n",
            "Epoch 00015: val_loss improved from 11.58156 to 11.54074, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 10.3038 - val_loss: 11.5407\n",
            "Epoch 16/10000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 11.54074\n",
            "231/231 - 1s - loss: 10.1407 - val_loss: 11.8208\n",
            "Epoch 17/10000\n",
            "\n",
            "Epoch 00017: val_loss improved from 11.54074 to 11.10814, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 9.9744 - val_loss: 11.1081\n",
            "Epoch 18/10000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 11.10814\n",
            "231/231 - 1s - loss: 9.8123 - val_loss: 11.1477\n",
            "Epoch 19/10000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 11.10814\n",
            "231/231 - 1s - loss: 9.6275 - val_loss: 11.1624\n",
            "Epoch 20/10000\n",
            "\n",
            "Epoch 00020: val_loss improved from 11.10814 to 10.57646, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 9.4168 - val_loss: 10.5765\n",
            "Epoch 21/10000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 10.57646\n",
            "231/231 - 1s - loss: 9.1150 - val_loss: 10.7557\n",
            "Epoch 22/10000\n",
            "\n",
            "Epoch 00022: val_loss improved from 10.57646 to 10.37879, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 8.9139 - val_loss: 10.3788\n",
            "Epoch 23/10000\n",
            "\n",
            "Epoch 00023: val_loss improved from 10.37879 to 10.24123, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 8.6683 - val_loss: 10.2412\n",
            "Epoch 24/10000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 10.24123\n",
            "231/231 - 1s - loss: 8.4640 - val_loss: 10.3011\n",
            "Epoch 25/10000\n",
            "\n",
            "Epoch 00025: val_loss improved from 10.24123 to 9.91203, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 8.2391 - val_loss: 9.9120\n",
            "Epoch 26/10000\n",
            "\n",
            "Epoch 00026: val_loss improved from 9.91203 to 9.70938, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 8.0723 - val_loss: 9.7094\n",
            "Epoch 27/10000\n",
            "\n",
            "Epoch 00027: val_loss improved from 9.70938 to 9.31208, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 7.8142 - val_loss: 9.3121\n",
            "Epoch 28/10000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 9.31208\n",
            "231/231 - 1s - loss: 7.6577 - val_loss: 9.5434\n",
            "Epoch 29/10000\n",
            "\n",
            "Epoch 00029: val_loss improved from 9.31208 to 9.21463, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 7.5732 - val_loss: 9.2146\n",
            "Epoch 30/10000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 9.21463\n",
            "231/231 - 1s - loss: 7.2745 - val_loss: 9.3131\n",
            "Epoch 31/10000\n",
            "\n",
            "Epoch 00031: val_loss improved from 9.21463 to 8.92640, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 7.1497 - val_loss: 8.9264\n",
            "Epoch 32/10000\n",
            "\n",
            "Epoch 00032: val_loss improved from 8.92640 to 8.73673, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 6.9645 - val_loss: 8.7367\n",
            "Epoch 33/10000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 8.73673\n",
            "231/231 - 1s - loss: 6.8130 - val_loss: 8.8167\n",
            "Epoch 34/10000\n",
            "\n",
            "Epoch 00034: val_loss improved from 8.73673 to 8.68214, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 6.6441 - val_loss: 8.6821\n",
            "Epoch 35/10000\n",
            "\n",
            "Epoch 00035: val_loss improved from 8.68214 to 8.54544, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 6.3964 - val_loss: 8.5454\n",
            "Epoch 36/10000\n",
            "\n",
            "Epoch 00036: val_loss improved from 8.54544 to 8.04430, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 6.3779 - val_loss: 8.0443\n",
            "Epoch 37/10000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 8.04430\n",
            "231/231 - 1s - loss: 6.1176 - val_loss: 8.1149\n",
            "Epoch 38/10000\n",
            "\n",
            "Epoch 00038: val_loss improved from 8.04430 to 7.82087, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.9494 - val_loss: 7.8209\n",
            "Epoch 39/10000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.82087\n",
            "231/231 - 1s - loss: 5.8807 - val_loss: 7.8257\n",
            "Epoch 40/10000\n",
            "\n",
            "Epoch 00040: val_loss improved from 7.82087 to 7.70921, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.7599 - val_loss: 7.7092\n",
            "Epoch 41/10000\n",
            "\n",
            "Epoch 00041: val_loss improved from 7.70921 to 7.50169, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.6338 - val_loss: 7.5017\n",
            "Epoch 42/10000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.50169\n",
            "231/231 - 1s - loss: 5.5206 - val_loss: 7.5799\n",
            "Epoch 43/10000\n",
            "\n",
            "Epoch 00043: val_loss improved from 7.50169 to 7.47856, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.4375 - val_loss: 7.4786\n",
            "Epoch 44/10000\n",
            "\n",
            "Epoch 00044: val_loss improved from 7.47856 to 7.40208, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.3192 - val_loss: 7.4021\n",
            "Epoch 45/10000\n",
            "\n",
            "Epoch 00045: val_loss improved from 7.40208 to 7.20349, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.2553 - val_loss: 7.2035\n",
            "Epoch 46/10000\n",
            "\n",
            "Epoch 00046: val_loss improved from 7.20349 to 7.01564, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 5.0854 - val_loss: 7.0156\n",
            "Epoch 47/10000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 7.01564\n",
            "231/231 - 1s - loss: 4.9740 - val_loss: 7.0694\n",
            "Epoch 48/10000\n",
            "\n",
            "Epoch 00048: val_loss improved from 7.01564 to 6.90509, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.9367 - val_loss: 6.9051\n",
            "Epoch 49/10000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 6.90509\n",
            "231/231 - 1s - loss: 4.8073 - val_loss: 6.9716\n",
            "Epoch 50/10000\n",
            "\n",
            "Epoch 00050: val_loss improved from 6.90509 to 6.80666, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.7509 - val_loss: 6.8067\n",
            "Epoch 51/10000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 6.80666\n",
            "231/231 - 1s - loss: 4.7263 - val_loss: 6.8510\n",
            "Epoch 52/10000\n",
            "\n",
            "Epoch 00052: val_loss improved from 6.80666 to 6.69512, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.6194 - val_loss: 6.6951\n",
            "Epoch 53/10000\n",
            "\n",
            "Epoch 00053: val_loss improved from 6.69512 to 6.56885, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.5124 - val_loss: 6.5688\n",
            "Epoch 54/10000\n",
            "\n",
            "Epoch 00054: val_loss improved from 6.56885 to 6.44842, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.4381 - val_loss: 6.4484\n",
            "Epoch 55/10000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 6.44842\n",
            "231/231 - 1s - loss: 4.3811 - val_loss: 6.4888\n",
            "Epoch 56/10000\n",
            "\n",
            "Epoch 00056: val_loss improved from 6.44842 to 6.38587, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.3492 - val_loss: 6.3859\n",
            "Epoch 57/10000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 6.38587\n",
            "231/231 - 1s - loss: 4.2754 - val_loss: 6.4772\n",
            "Epoch 58/10000\n",
            "\n",
            "Epoch 00058: val_loss improved from 6.38587 to 6.26109, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.1872 - val_loss: 6.2611\n",
            "Epoch 59/10000\n",
            "\n",
            "Epoch 00059: val_loss improved from 6.26109 to 6.07467, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.1859 - val_loss: 6.0747\n",
            "Epoch 60/10000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 6.07467\n",
            "231/231 - 1s - loss: 4.1085 - val_loss: 6.0832\n",
            "Epoch 61/10000\n",
            "\n",
            "Epoch 00061: val_loss improved from 6.07467 to 6.01829, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.0745 - val_loss: 6.0183\n",
            "Epoch 62/10000\n",
            "\n",
            "Epoch 00062: val_loss improved from 6.01829 to 5.91901, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 4.0295 - val_loss: 5.9190\n",
            "Epoch 63/10000\n",
            "\n",
            "Epoch 00063: val_loss improved from 5.91901 to 5.83134, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.9397 - val_loss: 5.8313\n",
            "Epoch 64/10000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 5.83134\n",
            "231/231 - 1s - loss: 3.9044 - val_loss: 5.8903\n",
            "Epoch 65/10000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 5.83134\n",
            "231/231 - 1s - loss: 3.9404 - val_loss: 6.0117\n",
            "Epoch 66/10000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 5.83134\n",
            "231/231 - 1s - loss: 3.8591 - val_loss: 5.8764\n",
            "Epoch 67/10000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 5.83134\n",
            "231/231 - 1s - loss: 3.8335 - val_loss: 5.9843\n",
            "Epoch 68/10000\n",
            "\n",
            "Epoch 00068: val_loss improved from 5.83134 to 5.74038, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.7527 - val_loss: 5.7404\n",
            "Epoch 69/10000\n",
            "\n",
            "Epoch 00069: val_loss improved from 5.74038 to 5.62654, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.7305 - val_loss: 5.6265\n",
            "Epoch 70/10000\n",
            "\n",
            "Epoch 00070: val_loss improved from 5.62654 to 5.58568, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.6806 - val_loss: 5.5857\n",
            "Epoch 71/10000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 5.58568\n",
            "231/231 - 1s - loss: 3.6383 - val_loss: 5.6451\n",
            "Epoch 72/10000\n",
            "\n",
            "Epoch 00072: val_loss improved from 5.58568 to 5.53544, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.6446 - val_loss: 5.5354\n",
            "Epoch 73/10000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 5.53544\n",
            "231/231 - 1s - loss: 3.5726 - val_loss: 5.7059\n",
            "Epoch 74/10000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 5.53544\n",
            "231/231 - 1s - loss: 3.5417 - val_loss: 5.5993\n",
            "Epoch 75/10000\n",
            "\n",
            "Epoch 00075: val_loss improved from 5.53544 to 5.52354, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.5036 - val_loss: 5.5235\n",
            "Epoch 76/10000\n",
            "\n",
            "Epoch 00076: val_loss improved from 5.52354 to 5.44470, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.5037 - val_loss: 5.4447\n",
            "Epoch 77/10000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 5.44470\n",
            "231/231 - 1s - loss: 3.4776 - val_loss: 5.7342\n",
            "Epoch 78/10000\n",
            "\n",
            "Epoch 00078: val_loss improved from 5.44470 to 5.36802, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.4127 - val_loss: 5.3680\n",
            "Epoch 79/10000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 5.36802\n",
            "231/231 - 1s - loss: 3.4060 - val_loss: 5.4726\n",
            "Epoch 80/10000\n",
            "\n",
            "Epoch 00080: val_loss improved from 5.36802 to 5.34082, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.3542 - val_loss: 5.3408\n",
            "Epoch 81/10000\n",
            "\n",
            "Epoch 00081: val_loss improved from 5.34082 to 5.27296, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.3616 - val_loss: 5.2730\n",
            "Epoch 82/10000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 5.27296\n",
            "231/231 - 1s - loss: 3.2829 - val_loss: 5.3676\n",
            "Epoch 83/10000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 5.27296\n",
            "231/231 - 1s - loss: 3.3156 - val_loss: 5.3474\n",
            "Epoch 84/10000\n",
            "\n",
            "Epoch 00084: val_loss improved from 5.27296 to 5.24099, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.2284 - val_loss: 5.2410\n",
            "Epoch 85/10000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 5.24099\n",
            "231/231 - 1s - loss: 3.2249 - val_loss: 5.2637\n",
            "Epoch 86/10000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 5.24099\n",
            "231/231 - 1s - loss: 3.1666 - val_loss: 5.3189\n",
            "Epoch 87/10000\n",
            "\n",
            "Epoch 00087: val_loss improved from 5.24099 to 5.21353, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.1971 - val_loss: 5.2135\n",
            "Epoch 88/10000\n",
            "\n",
            "Epoch 00088: val_loss improved from 5.21353 to 5.14496, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.1517 - val_loss: 5.1450\n",
            "Epoch 89/10000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 5.14496\n",
            "231/231 - 1s - loss: 3.1308 - val_loss: 5.3055\n",
            "Epoch 90/10000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 5.14496\n",
            "231/231 - 1s - loss: 3.0877 - val_loss: 5.3689\n",
            "Epoch 91/10000\n",
            "\n",
            "Epoch 00091: val_loss improved from 5.14496 to 5.13020, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 3.0823 - val_loss: 5.1302\n",
            "Epoch 92/10000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 5.13020\n",
            "231/231 - 1s - loss: 3.0605 - val_loss: 5.1793\n",
            "Epoch 93/10000\n",
            "\n",
            "Epoch 00093: val_loss improved from 5.13020 to 5.11820, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.9922 - val_loss: 5.1182\n",
            "Epoch 94/10000\n",
            "\n",
            "Epoch 00094: val_loss improved from 5.11820 to 5.01793, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.9957 - val_loss: 5.0179\n",
            "Epoch 95/10000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 5.01793\n",
            "231/231 - 1s - loss: 2.9851 - val_loss: 5.0941\n",
            "Epoch 96/10000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 5.01793\n",
            "231/231 - 1s - loss: 2.9774 - val_loss: 5.0707\n",
            "Epoch 97/10000\n",
            "\n",
            "Epoch 00097: val_loss improved from 5.01793 to 4.97973, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.9599 - val_loss: 4.9797\n",
            "Epoch 98/10000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 4.97973\n",
            "231/231 - 1s - loss: 2.9256 - val_loss: 4.9815\n",
            "Epoch 99/10000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 4.97973\n",
            "231/231 - 1s - loss: 2.9155 - val_loss: 5.0339\n",
            "Epoch 100/10000\n",
            "\n",
            "Epoch 00100: val_loss improved from 4.97973 to 4.90673, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.8955 - val_loss: 4.9067\n",
            "Epoch 101/10000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 4.90673\n",
            "231/231 - 1s - loss: 2.8882 - val_loss: 5.0465\n",
            "Epoch 102/10000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 4.90673\n",
            "231/231 - 1s - loss: 2.8743 - val_loss: 4.9634\n",
            "Epoch 103/10000\n",
            "\n",
            "Epoch 00103: val_loss improved from 4.90673 to 4.88673, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.8616 - val_loss: 4.8867\n",
            "Epoch 104/10000\n",
            "\n",
            "Epoch 00104: val_loss improved from 4.88673 to 4.87698, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.8202 - val_loss: 4.8770\n",
            "Epoch 105/10000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 4.87698\n",
            "231/231 - 1s - loss: 2.7987 - val_loss: 4.9002\n",
            "Epoch 106/10000\n",
            "\n",
            "Epoch 00106: val_loss improved from 4.87698 to 4.85964, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.7787 - val_loss: 4.8596\n",
            "Epoch 107/10000\n",
            "\n",
            "Epoch 00107: val_loss improved from 4.85964 to 4.85804, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.8058 - val_loss: 4.8580\n",
            "Epoch 108/10000\n",
            "\n",
            "Epoch 00108: val_loss improved from 4.85804 to 4.84784, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.7491 - val_loss: 4.8478\n",
            "Epoch 109/10000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 4.84784\n",
            "231/231 - 1s - loss: 2.7422 - val_loss: 4.8933\n",
            "Epoch 110/10000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 4.84784\n",
            "231/231 - 1s - loss: 2.7278 - val_loss: 4.8938\n",
            "Epoch 111/10000\n",
            "\n",
            "Epoch 00111: val_loss improved from 4.84784 to 4.83112, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.7222 - val_loss: 4.8311\n",
            "Epoch 112/10000\n",
            "\n",
            "Epoch 00112: val_loss improved from 4.83112 to 4.79269, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.6710 - val_loss: 4.7927\n",
            "Epoch 113/10000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 4.79269\n",
            "231/231 - 1s - loss: 2.6636 - val_loss: 4.9156\n",
            "Epoch 114/10000\n",
            "\n",
            "Epoch 00114: val_loss improved from 4.79269 to 4.77337, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.6616 - val_loss: 4.7734\n",
            "Epoch 115/10000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 4.77337\n",
            "231/231 - 1s - loss: 2.6692 - val_loss: 4.8708\n",
            "Epoch 116/10000\n",
            "\n",
            "Epoch 00116: val_loss improved from 4.77337 to 4.72979, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.6477 - val_loss: 4.7298\n",
            "Epoch 117/10000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 4.72979\n",
            "231/231 - 1s - loss: 2.6255 - val_loss: 4.7529\n",
            "Epoch 118/10000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 4.72979\n",
            "231/231 - 1s - loss: 2.6030 - val_loss: 4.7433\n",
            "Epoch 119/10000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 4.72979\n",
            "231/231 - 1s - loss: 2.5869 - val_loss: 4.7604\n",
            "Epoch 120/10000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 4.72979\n",
            "231/231 - 1s - loss: 2.5847 - val_loss: 4.7319\n",
            "Epoch 121/10000\n",
            "\n",
            "Epoch 00121: val_loss improved from 4.72979 to 4.69551, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.5679 - val_loss: 4.6955\n",
            "Epoch 122/10000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 4.69551\n",
            "231/231 - 1s - loss: 2.5491 - val_loss: 4.7449\n",
            "Epoch 123/10000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 4.69551\n",
            "231/231 - 1s - loss: 2.5396 - val_loss: 4.6966\n",
            "Epoch 124/10000\n",
            "\n",
            "Epoch 00124: val_loss improved from 4.69551 to 4.63066, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.4978 - val_loss: 4.6307\n",
            "Epoch 125/10000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.5142 - val_loss: 4.7507\n",
            "Epoch 126/10000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.4912 - val_loss: 4.6900\n",
            "Epoch 127/10000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.4797 - val_loss: 4.6646\n",
            "Epoch 128/10000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.4694 - val_loss: 4.6739\n",
            "Epoch 129/10000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.4680 - val_loss: 4.6620\n",
            "Epoch 130/10000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 4.63066\n",
            "231/231 - 1s - loss: 2.4769 - val_loss: 4.6312\n",
            "Epoch 131/10000\n",
            "\n",
            "Epoch 00131: val_loss improved from 4.63066 to 4.60519, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.4511 - val_loss: 4.6052\n",
            "Epoch 132/10000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 4.60519\n",
            "231/231 - 1s - loss: 2.4249 - val_loss: 4.6436\n",
            "Epoch 133/10000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 4.60519\n",
            "231/231 - 1s - loss: 2.4070 - val_loss: 4.6606\n",
            "Epoch 134/10000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 4.60519\n",
            "231/231 - 1s - loss: 2.3992 - val_loss: 4.6917\n",
            "Epoch 135/10000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 4.60519\n",
            "231/231 - 1s - loss: 2.4020 - val_loss: 4.6144\n",
            "Epoch 136/10000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 4.60519\n",
            "231/231 - 1s - loss: 2.3968 - val_loss: 4.6419\n",
            "Epoch 137/10000\n",
            "\n",
            "Epoch 00137: val_loss improved from 4.60519 to 4.60313, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.3713 - val_loss: 4.6031\n",
            "Epoch 138/10000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 4.60313\n",
            "231/231 - 1s - loss: 2.3810 - val_loss: 4.6073\n",
            "Epoch 139/10000\n",
            "\n",
            "Epoch 00139: val_loss improved from 4.60313 to 4.52993, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.3429 - val_loss: 4.5299\n",
            "Epoch 140/10000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 4.52993\n",
            "231/231 - 1s - loss: 2.3365 - val_loss: 4.5530\n",
            "Epoch 141/10000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 4.52993\n",
            "231/231 - 1s - loss: 2.3196 - val_loss: 4.7126\n",
            "Epoch 142/10000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 4.52993\n",
            "231/231 - 1s - loss: 2.3011 - val_loss: 4.5452\n",
            "Epoch 143/10000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 4.52993\n",
            "231/231 - 1s - loss: 2.2945 - val_loss: 4.6000\n",
            "Epoch 144/10000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 4.52993\n",
            "231/231 - 1s - loss: 2.3160 - val_loss: 4.5472\n",
            "Epoch 145/10000\n",
            "\n",
            "Epoch 00145: val_loss improved from 4.52993 to 4.52634, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.2926 - val_loss: 4.5263\n",
            "Epoch 146/10000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 4.52634\n",
            "231/231 - 1s - loss: 2.2682 - val_loss: 4.6367\n",
            "Epoch 147/10000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 4.52634\n",
            "231/231 - 1s - loss: 2.2893 - val_loss: 4.5841\n",
            "Epoch 148/10000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 4.52634\n",
            "231/231 - 1s - loss: 2.2691 - val_loss: 4.5406\n",
            "Epoch 149/10000\n",
            "\n",
            "Epoch 00149: val_loss improved from 4.52634 to 4.48666, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.2536 - val_loss: 4.4867\n",
            "Epoch 150/10000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 4.48666\n",
            "231/231 - 1s - loss: 2.2390 - val_loss: 4.5393\n",
            "Epoch 151/10000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 4.48666\n",
            "231/231 - 0s - loss: 2.2464 - val_loss: 4.5987\n",
            "Epoch 152/10000\n",
            "\n",
            "Epoch 00152: val_loss improved from 4.48666 to 4.48072, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.2377 - val_loss: 4.4807\n",
            "Epoch 153/10000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 4.48072\n",
            "231/231 - 1s - loss: 2.2012 - val_loss: 4.5304\n",
            "Epoch 154/10000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 4.48072\n",
            "231/231 - 1s - loss: 2.2037 - val_loss: 4.5185\n",
            "Epoch 155/10000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 4.48072\n",
            "231/231 - 1s - loss: 2.1876 - val_loss: 4.5616\n",
            "Epoch 156/10000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 4.48072\n",
            "231/231 - 1s - loss: 2.1785 - val_loss: 4.5671\n",
            "Epoch 157/10000\n",
            "\n",
            "Epoch 00157: val_loss improved from 4.48072 to 4.46760, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.1890 - val_loss: 4.4676\n",
            "Epoch 158/10000\n",
            "\n",
            "Epoch 00158: val_loss improved from 4.46760 to 4.45206, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.1882 - val_loss: 4.4521\n",
            "Epoch 159/10000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1774 - val_loss: 4.4710\n",
            "Epoch 160/10000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1706 - val_loss: 4.4665\n",
            "Epoch 161/10000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1585 - val_loss: 4.4910\n",
            "Epoch 162/10000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1173 - val_loss: 4.4902\n",
            "Epoch 163/10000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1392 - val_loss: 4.5322\n",
            "Epoch 164/10000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1041 - val_loss: 4.4585\n",
            "Epoch 165/10000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 4.45206\n",
            "231/231 - 1s - loss: 2.1231 - val_loss: 4.4801\n",
            "Epoch 166/10000\n",
            "\n",
            "Epoch 00166: val_loss improved from 4.45206 to 4.44639, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.1168 - val_loss: 4.4464\n",
            "Epoch 167/10000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 4.44639\n",
            "231/231 - 1s - loss: 2.1090 - val_loss: 4.4728\n",
            "Epoch 168/10000\n",
            "\n",
            "Epoch 00168: val_loss improved from 4.44639 to 4.37964, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.0976 - val_loss: 4.3796\n",
            "Epoch 169/10000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 4.37964\n",
            "231/231 - 1s - loss: 2.1052 - val_loss: 4.4589\n",
            "Epoch 170/10000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 4.37964\n",
            "231/231 - 1s - loss: 2.0522 - val_loss: 4.4281\n",
            "Epoch 171/10000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 4.37964\n",
            "231/231 - 1s - loss: 2.0700 - val_loss: 4.4504\n",
            "Epoch 172/10000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 4.37964\n",
            "231/231 - 1s - loss: 2.0352 - val_loss: 4.4384\n",
            "Epoch 173/10000\n",
            "\n",
            "Epoch 00173: val_loss improved from 4.37964 to 4.37636, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.0411 - val_loss: 4.3764\n",
            "Epoch 174/10000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 4.37636\n",
            "231/231 - 1s - loss: 2.0333 - val_loss: 4.4164\n",
            "Epoch 175/10000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 4.37636\n",
            "231/231 - 1s - loss: 2.0545 - val_loss: 4.4039\n",
            "Epoch 176/10000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 4.37636\n",
            "231/231 - 1s - loss: 2.0615 - val_loss: 4.4092\n",
            "Epoch 177/10000\n",
            "\n",
            "Epoch 00177: val_loss improved from 4.37636 to 4.36783, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 2.0205 - val_loss: 4.3678\n",
            "Epoch 178/10000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 4.36783\n",
            "231/231 - 1s - loss: 2.0176 - val_loss: 4.4119\n",
            "Epoch 179/10000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 4.36783\n",
            "231/231 - 1s - loss: 2.0176 - val_loss: 4.4199\n",
            "Epoch 180/10000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 4.36783\n",
            "231/231 - 1s - loss: 2.0179 - val_loss: 4.3765\n",
            "Epoch 181/10000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 4.36783\n",
            "231/231 - 1s - loss: 1.9917 - val_loss: 4.4274\n",
            "Epoch 182/10000\n",
            "\n",
            "Epoch 00182: val_loss improved from 4.36783 to 4.36155, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.9866 - val_loss: 4.3616\n",
            "Epoch 183/10000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 4.36155\n",
            "231/231 - 1s - loss: 1.9860 - val_loss: 4.4264\n",
            "Epoch 184/10000\n",
            "\n",
            "Epoch 00184: val_loss improved from 4.36155 to 4.34633, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.9893 - val_loss: 4.3463\n",
            "Epoch 185/10000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9594 - val_loss: 4.5367\n",
            "Epoch 186/10000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9466 - val_loss: 4.3746\n",
            "Epoch 187/10000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9596 - val_loss: 4.3813\n",
            "Epoch 188/10000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9875 - val_loss: 4.4210\n",
            "Epoch 189/10000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9317 - val_loss: 4.3675\n",
            "Epoch 190/10000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9580 - val_loss: 4.3472\n",
            "Epoch 191/10000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 4.34633\n",
            "231/231 - 1s - loss: 1.9472 - val_loss: 4.3700\n",
            "Epoch 192/10000\n",
            "\n",
            "Epoch 00192: val_loss improved from 4.34633 to 4.32121, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.9224 - val_loss: 4.3212\n",
            "Epoch 193/10000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9089 - val_loss: 4.3267\n",
            "Epoch 194/10000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9205 - val_loss: 4.3679\n",
            "Epoch 195/10000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9301 - val_loss: 4.3984\n",
            "Epoch 196/10000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9119 - val_loss: 4.4305\n",
            "Epoch 197/10000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9002 - val_loss: 4.4244\n",
            "Epoch 198/10000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 4.32121\n",
            "231/231 - 1s - loss: 1.9002 - val_loss: 4.3791\n",
            "Epoch 199/10000\n",
            "\n",
            "Epoch 00199: val_loss improved from 4.32121 to 4.30387, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.8884 - val_loss: 4.3039\n",
            "Epoch 200/10000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8908 - val_loss: 4.4134\n",
            "Epoch 201/10000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8940 - val_loss: 4.3645\n",
            "Epoch 202/10000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8540 - val_loss: 4.3574\n",
            "Epoch 203/10000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8695 - val_loss: 4.3066\n",
            "Epoch 204/10000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8519 - val_loss: 4.3610\n",
            "Epoch 205/10000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8793 - val_loss: 4.3252\n",
            "Epoch 206/10000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8397 - val_loss: 4.3230\n",
            "Epoch 207/10000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8371 - val_loss: 4.3375\n",
            "Epoch 208/10000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8121 - val_loss: 4.3776\n",
            "Epoch 209/10000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8219 - val_loss: 4.3343\n",
            "Epoch 210/10000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 4.30387\n",
            "231/231 - 0s - loss: 1.8252 - val_loss: 4.3493\n",
            "Epoch 211/10000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8068 - val_loss: 4.3935\n",
            "Epoch 212/10000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8190 - val_loss: 4.3284\n",
            "Epoch 213/10000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.7924 - val_loss: 4.3185\n",
            "Epoch 214/10000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 4.30387\n",
            "231/231 - 1s - loss: 1.8172 - val_loss: 4.3830\n",
            "Epoch 215/10000\n",
            "\n",
            "Epoch 00215: val_loss improved from 4.30387 to 4.29881, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.8094 - val_loss: 4.2988\n",
            "Epoch 216/10000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 4.29881\n",
            "231/231 - 1s - loss: 1.7796 - val_loss: 4.3386\n",
            "Epoch 217/10000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 4.29881\n",
            "231/231 - 1s - loss: 1.7782 - val_loss: 4.3788\n",
            "Epoch 218/10000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 4.29881\n",
            "231/231 - 1s - loss: 1.7804 - val_loss: 4.3098\n",
            "Epoch 219/10000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 4.29881\n",
            "231/231 - 1s - loss: 1.7615 - val_loss: 4.3406\n",
            "Epoch 220/10000\n",
            "\n",
            "Epoch 00220: val_loss improved from 4.29881 to 4.29157, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.7876 - val_loss: 4.2916\n",
            "Epoch 221/10000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7825 - val_loss: 4.3548\n",
            "Epoch 222/10000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7793 - val_loss: 4.3203\n",
            "Epoch 223/10000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7665 - val_loss: 4.3297\n",
            "Epoch 224/10000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7529 - val_loss: 4.3271\n",
            "Epoch 225/10000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7439 - val_loss: 4.3380\n",
            "Epoch 226/10000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7568 - val_loss: 4.3740\n",
            "Epoch 227/10000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7226 - val_loss: 4.3648\n",
            "Epoch 228/10000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7327 - val_loss: 4.2992\n",
            "Epoch 229/10000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7380 - val_loss: 4.3298\n",
            "Epoch 230/10000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7132 - val_loss: 4.3335\n",
            "Epoch 231/10000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7287 - val_loss: 4.3660\n",
            "Epoch 232/10000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7309 - val_loss: 4.3435\n",
            "Epoch 233/10000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 4.29157\n",
            "231/231 - 1s - loss: 1.7135 - val_loss: 4.3345\n",
            "Epoch 234/10000\n",
            "\n",
            "Epoch 00234: val_loss improved from 4.29157 to 4.26103, saving model to weights.hdf5\n",
            "231/231 - 1s - loss: 1.7239 - val_loss: 4.2610\n",
            "Epoch 235/10000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.7009 - val_loss: 4.2994\n",
            "Epoch 236/10000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6766 - val_loss: 4.3369\n",
            "Epoch 237/10000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6798 - val_loss: 4.3132\n",
            "Epoch 238/10000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6726 - val_loss: 4.3040\n",
            "Epoch 239/10000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6840 - val_loss: 4.3002\n",
            "Epoch 240/10000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6836 - val_loss: 4.2991\n",
            "Epoch 241/10000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6957 - val_loss: 4.3109\n",
            "Epoch 242/10000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6614 - val_loss: 4.2888\n",
            "Epoch 243/10000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6756 - val_loss: 4.2934\n",
            "Epoch 244/10000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6550 - val_loss: 4.3337\n",
            "Epoch 245/10000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6614 - val_loss: 4.2844\n",
            "Epoch 246/10000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6690 - val_loss: 4.3486\n",
            "Epoch 247/10000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6392 - val_loss: 4.3611\n",
            "Epoch 248/10000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6237 - val_loss: 4.3181\n",
            "Epoch 249/10000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6189 - val_loss: 4.3926\n",
            "Epoch 250/10000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6374 - val_loss: 4.3116\n",
            "Epoch 251/10000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6343 - val_loss: 4.2770\n",
            "Epoch 252/10000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6350 - val_loss: 4.2958\n",
            "Epoch 253/10000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6200 - val_loss: 4.2876\n",
            "Epoch 254/10000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 4.26103\n",
            "231/231 - 1s - loss: 1.6151 - val_loss: 4.3344\n",
            "Epoch 00254: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiUHvs3vas4C",
        "outputId": "dd717d23-45f9-48e1-f221-da7fc4f3df57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "model = load_model('weights.hdf5')\n",
        "preds = model.predict(x_test) #The test's last input is 10.27's avarage, the first output is 10.28\n",
        "print(\"The temp on Oct 28 will be:\")\n",
        "print(preds[0,0])\n",
        "print('\\n')\n",
        "print(\"The temp on Nov 3 will be:\")\n",
        "print(preds[0,6])\n",
        "print('\\n')\n",
        "print(\"The temp on Nov 24 will be:\")\n",
        "print(preds[0,27])\n",
        "t=np.linspace(0,predto-1,predto)\n",
        "plt.plot(t,preds[0,]) #we can check all the 31 data here in a diagram\n",
        "plt.show() # as we can see the network learned what i wanted, it recognised the next month is november, the temperatures are decrasing"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The temp on Oct 28 will be:\n",
            "10.7094145\n",
            "\n",
            "\n",
            "The temp on Nov 3 will be:\n",
            "11.39303\n",
            "\n",
            "\n",
            "The temp on Nov 24 will be:\n",
            "5.1654625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}