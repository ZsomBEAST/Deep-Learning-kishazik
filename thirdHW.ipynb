{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harmadik_kishazi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWSQsEDgyipqfnnLeiKohz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZsomBEAST/Deep-Learning-kishazik/blob/main/thirdHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bZUf52Hwy_O"
      },
      "source": [
        "from tensorflow.keras.models import Sequential                  #imports to make keras model\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import Callback \n",
        "from tensorflow.keras.optimizers import SGD #we will use stohastic gradient \n",
        "import numpy as np\n",
        "from keras.models import load_model  #we will save the best model and in the end reproduce it\n",
        "from sklearn import preprocessing #for normalization\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "import csv # for reading csv\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpigQOjcasyX"
      },
      "source": [
        "predfrom=365 #we can set how much previous temperatuers are the input -> I chose 1 year beacuse i want the network to recognise the changing of the seasons\n",
        "predto=31 # the out put is 31 days = 1 moth\n",
        "avgT=[] #list for the database\n",
        "with open('hom_csv.csv') as csv_file: #load the csv (I have loaded all the data from 'http://idojarasbudapest.hu/archivalt-idojaras' to a CSV file)\n",
        "    csv_reader = csv.reader(csv_file, delimiter=';') # ';' is the separating character\n",
        "    line_count = 0 # line counter\n",
        "    for row in csv_reader:\n",
        "        if(not(line_count%2)): #the tempreatures are in every 2nd row\n",
        "            avgT.append((float(row[1])+float(row[2]))/2) #mean=(min+max)/2\n",
        "            #print(row[1],row[2])\n",
        "        line_count=line_count+1\n",
        "\n",
        "#we predict the next 31 day from the last 60 day\n",
        "avgT_array=np.array(avgT) #make np.array from the list\n",
        "x_all=[]\n",
        "y_all=[] #lists for the input and output datas of the netowrk\n",
        "#making the inputs and the labels\n",
        "for i in range(predfrom,(len(avgT_array)-predto+1)):\n",
        "    x_all.append(avgT_array[(i-predfrom):i]) #the input is the pervious 365 day\n",
        "    y_all.append(avgT_array[i:(i+predto)]) #the label for the cost is the next 31 day\n",
        "\n",
        "x_test_pre=avgT_array[-predfrom-1:-1] # we will test the prediction the the future, what does it predict for the next month (ofc we are not able to calculate the cost right now)\n",
        "\n",
        "print(x_test_pre.shape) #the last year\n",
        "x_test=np.zeros([1,predfrom]) #make a 2D array from the 1D array to match the networks input nd the\n",
        "x_test[0]=x_test_pre \n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8K4EUJTas1U"
      },
      "source": [
        "print(len(avgT_array)) #the length of the original avarage temperature dataset\n",
        "print(len(x_all)) #length of the netowrks input dataset\n",
        "print(len(y_all)) #length of the netowrks expectation dataset\n",
        "\n",
        "x_all=np.array(x_all) #We had a list with 1D np.array elemnts so far. Now we make 2D array from that.\n",
        "y_all=np.array(y_all)\n",
        "\n",
        "#make the order of the array random ->we dont want that the order influence the learning\n",
        "randperm = np.random.permutation(len(x_all))\n",
        "x_all, y_all = x_all[randperm], y_all[randperm]\n",
        "\n",
        "#split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=0.2, shuffle=True) # 80% of the data is the train data, 20% is for validation\n",
        "#the test will be the future predict:)\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(x_train) #normalazie all the inputs by the x_train\n",
        "x_train = scaler.transform(x_train)\n",
        "x_valid = scaler.transform(x_valid)\n",
        "x_test= scaler.transform(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_valid.shape)\n",
        "print(y_valid.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKZ4_-t4ssb"
      },
      "source": [
        "patience=20\n",
        "early_stopping=EarlyStopping(patience=patience, verbose=1) #we are using earlystoping, the laerning usually stops the intesive learning about 250- 30 epochs\n",
        "checkpointer=ModelCheckpoint(filepath='weights.hdf5', save_best_only=True, verbose=1) #we save the networks wights when the cost is the smallest\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10000, input_shape=(predfrom,))) #we use only 1 hiden layer with 10000 neuron, I have tried lots of variations, this variation has got the best loss/source in my opp.\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5)) # we apply dropout as well\n",
        "model.add(Dense(predto, activation='linear'))\n",
        "\n",
        "\n",
        "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) # set the erst of the hyperparameters (by iterations)\n",
        "model.compile(loss='mse', optimizer=sgd) \n",
        "history=model.fit(x_train,y_train,epochs=10000, #we say 10000 epoch but it usually earlystops around 300 \n",
        "                  batch_size=10, # we use very small batches\n",
        "                  verbose=2,\n",
        "                  validation_data=(x_valid, y_valid),\n",
        "                  callbacks=[checkpointer, early_stopping])\n",
        "\n",
        "\n",
        "model = load_model('weights.hdf5') #reproduce the best model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiUHvs3vas4C"
      },
      "source": [
        "model = load_model('weights.hdf5')\n",
        "preds = model.predict(x_test) #The test's last input is 10.27's avarage, the first output is 10.28\n",
        "print(\"The temp on Oct 28 will be:\")\n",
        "print(preds[0,0])\n",
        "print('\\n')\n",
        "print(\"The temp on Nov 3 will be:\")\n",
        "print(preds[0,6])\n",
        "print('\\n')\n",
        "print(\"The temp on Nov 24 will be:\")\n",
        "print(preds[0,27])\n",
        "t=np.linspace(0,predto-1,predto)\n",
        "plt.plot(t,preds[0,]) #we can check all the 31 data here in a diagram\n",
        "plt.show() # as we can see the network learned what i wanted, it recognised the next month is november, the temperatures are decrasing"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}